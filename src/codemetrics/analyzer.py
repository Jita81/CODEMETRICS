"""
AI-powered analytics engine using Claude 4
"""

import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path

try:
    import anthropic
except ImportError:
    raise ImportError("anthropic package not installed. Run: pip install anthropic")

from .config import Config

@dataclass
class AnalysisResult:
    """Container for analysis results"""
    performance_score: int
    quality_score: int
    security_score: int
    key_findings: List[str]
    recommendations: List[str]
    metrics: Dict[str, Any]
    timestamp: float
    
    def summary(self) -> str:
        """Generate a summary of the analysis"""
        return f"""
ðŸ“Š CodeMetrics Analysis Summary

Performance Score: {self.performance_score}/100
Quality Score: {self.quality_score}/100  
Security Score: {self.security_score}/100

Key Findings:
{chr(10).join(f"â€¢ {finding}" for finding in self.key_findings)}

Recommendations:
{chr(10).join(f"â€¢ {rec}" for rec in self.recommendations)}
        """
    
    def save(self, filepath: str) -> None:
        """Save results to file"""
        output_path = Path(filepath)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        data = {
            'performance_score': self.performance_score,
            'quality_score': self.quality_score,
            'security_score': self.security_score,
            'key_findings': self.key_findings,
            'recommendations': self.recommendations,
            'metrics': self.metrics,
            'timestamp': self.timestamp
        }
        
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)

class MetricsAnalyzer:
    """AI-powered analytics engine"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = anthropic.Anthropic(api_key=config.anthropic_api_key)
        
    def analyze(self, data: Dict[str, Any], analysis_type: str = "full") -> AnalysisResult:
        """Perform AI-powered analysis of project data"""
        
        prompt = self._build_analysis_prompt(data, analysis_type)
        
        try:
            response = self.client.messages.create(
                model=self.config.model,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            # Parse AI response into structured results
            ai_response = response.content[0].text
            return self._parse_analysis_response(ai_response, data)
            
        except Exception as e:
            # Fallback to basic analysis if AI fails
            return self._fallback_analysis(data, str(e))
    
    def _build_analysis_prompt(self, data: Dict[str, Any], analysis_type: str) -> str:
        """Build the analysis prompt for Claude"""
        
        base_prompt = f"""
You are CodeMetrics, an expert AI analyst for the Automated Agile Framework ecosystem. 
Analyze the following project data and provide insights for optimization.

Analysis Type: {analysis_type}

Project Data:
{json.dumps(data, indent=2, default=str)}

Ecosystem Context:
- This is part of the Automated Agile Framework with 4 components:
  1. Standardized Modules Framework (scaffolding)
  2. CodeCreate (AI generation with Claude 4)  
  3. CodeReview (quality & security analysis)
  4. CodeMetrics (analytics & optimization - this tool)

Please provide analysis in the following JSON format:
{{
    "performance_score": <0-100>,
    "quality_score": <0-100>,
    "security_score": <0-100>,
    "key_findings": [
        "Finding 1",
        "Finding 2",
        "..."
    ],
    "recommendations": [
        "Recommendation 1", 
        "Recommendation 2",
        "..."
    ],
    "detailed_metrics": {{
        "code_complexity": <score>,
        "test_coverage": <percentage>,
        "dependency_health": <score>,
        "performance_indicators": {{
            "response_time_ms": <value>,
            "memory_usage_mb": <value>,
            "cpu_utilization": <percentage>
        }},
        "ecosystem_integration": {{
            "framework_compatibility": <score>,
            "codecreate_pattern_adoption": <score>,
            "codereview_compliance": <score>
        }}
    }}
}}
        """
        
        if analysis_type == "performance":
            base_prompt += """
Focus specifically on:
- Response time optimization
- Resource utilization
- Scaling patterns
- Performance regression detection
            """
        elif analysis_type == "quality":
            base_prompt += """
Focus specifically on:
- Code maintainability
- Test coverage and quality
- Documentation completeness
- Best practice adherence
            """
        elif analysis_type == "security":
            base_prompt += """
Focus specifically on:
- Vulnerability detection
- Security best practices
- Compliance requirements (GDPR, SOC2, HIPAA)
- Threat pattern recognition
            """
        
        return base_prompt
    
    def _parse_analysis_response(self, ai_response: str, original_data: Dict[str, Any]) -> AnalysisResult:
        """Parse Claude's response into structured results"""
        
        try:
            # Try to extract JSON from the response
            start_idx = ai_response.find('{')
            end_idx = ai_response.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = ai_response[start_idx:end_idx]
                parsed = json.loads(json_str)
                
                return AnalysisResult(
                    performance_score=parsed.get('performance_score', 0),
                    quality_score=parsed.get('quality_score', 0),
                    security_score=parsed.get('security_score', 0),
                    key_findings=parsed.get('key_findings', []),
                    recommendations=parsed.get('recommendations', []),
                    metrics=parsed.get('detailed_metrics', {}),
                    timestamp=time.time()
                )
            else:
                # Fallback parsing if JSON not found
                return self._extract_scores_from_text(ai_response, original_data)
                
        except json.JSONDecodeError:
            return self._extract_scores_from_text(ai_response, original_data)
    
    def _extract_scores_from_text(self, text: str, data: Dict[str, Any]) -> AnalysisResult:
        """Extract scores and insights from free-form text response"""
        
        # Basic text parsing for scores
        performance_score = 75  # Default
        quality_score = 80
        security_score = 85
        
        # Simple keyword-based extraction
        key_findings = []
        recommendations = []
        
        lines = text.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if 'finding' in line.lower() or 'issue' in line.lower():
                current_section = 'findings'
            elif 'recommend' in line.lower() or 'suggest' in line.lower():
                current_section = 'recommendations'
            elif line.startswith('â€¢') or line.startswith('-') or line.startswith('*'):
                if current_section == 'findings':
                    key_findings.append(line[1:].strip())
                elif current_section == 'recommendations':
                    recommendations.append(line[1:].strip())
        
        return AnalysisResult(
            performance_score=performance_score,
            quality_score=quality_score,
            security_score=security_score,
            key_findings=key_findings or ["Analysis completed successfully"],
            recommendations=recommendations or ["Continue following best practices"],
            metrics={"analysis_type": "text_parsed", "original_data_size": len(str(data))},
            timestamp=time.time()
        )
    
    def _fallback_analysis(self, data: Dict[str, Any], error: str) -> AnalysisResult:
        """Provide basic analysis when AI is unavailable"""
        
        return AnalysisResult(
            performance_score=70,
            quality_score=75,
            security_score=80,
            key_findings=[
                f"Analysis completed with fallback method",
                f"AI analysis failed: {error[:100]}...",
                "Basic static analysis performed"
            ],
            recommendations=[
                "Verify Anthropic API key configuration",
                "Check network connectivity",
                "Review analysis parameters"
            ],
            metrics={
                "fallback_analysis": True,
                "error": error,
                "data_keys": list(data.keys()) if isinstance(data, dict) else []
            },
            timestamp=time.time()
        )
